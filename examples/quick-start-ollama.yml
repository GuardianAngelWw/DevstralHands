version: '3.8'

# Quick Start with Ollama - Simplest setup for beginners
# Usage: docker-compose -f examples/quick-start-ollama.yml up -d

services:
  ollama:
    image: ollama/ollama:latest
    container_name: devstral-ollama
    ports:
      - "${MODEL_SERVER_PORT:-11434}:11434"
    volumes:
      - ollama_data:/root/.ollama
      - ./models:/models:ro
    environment:
      - OLLAMA_HOST=0.0.0.0
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/api/tags"]
      interval: 30s
      timeout: 10s
      retries: 3

  openhands:
    image: docker.all-hands.dev/all-hands-ai/openhands:0.40
    container_name: devstral-openhands
    ports:
      - "${OPENHANDS_PORT:-3000}:3000"
    environment:
      - LLM_API_BASE=http://ollama:11434/v1
      - LLM_MODEL_NAME=${MODEL_NAME:-devstral}
      - SANDBOX_RUNTIME_CONTAINER_IMAGE=docker.all-hands.dev/all-hands-ai/runtime:0.40-nikolaik
      - SANDBOX_PERSIST_AFTER_START=false
      - WORKSPACE_MOUNT_PATH=/workspace
      # Added environment variables to solve API key and model prompt issues
      - LLM_API_KEY=sk-devstral-integration  # Provide a default API key to skip the prompt
      - SKIP_API_KEY_VALIDATION=true  # Skip API key validation
      - AUTO_CONNECT_MODEL=true  # Automatically connect to the model without prompting
      - DEFAULT_MODEL=devstral  # Set default model to use
      - DIRECT_MODEL_INTEGRATION=true  # Enable direct model integration
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
      - ${WORKSPACE_PATH:-./workspace}:/workspace
    depends_on:
      ollama:
        condition: service_healthy
    restart: unless-stopped

  # Optional: Web UI for Ollama management
  ollama-webui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: devstral-ollama-webui
    ports:
      - "${WEBUI_PORT:-8080}:8080"
    environment:
      - OLLAMA_BASE_URL=http://ollama:11434
      - WEBUI_NAME=Devstral Model Manager
    volumes:
      - ollama_webui_data:/app/backend/data
    depends_on:
      - ollama
    restart: unless-stopped

volumes:
  ollama_data:
  ollama_webui_data:

# After starting:
# 1. Load your model: docker exec -it devstral-ollama ollama create devstral -f /models/Modelfile
# 2. Access OpenHands: http://localhost:3000
# 3. Access Ollama Web UI: http://localhost:8080