version: '3.8'

# High-Performance Setup with llama.cpp
# Optimized for maximum performance with GPU support
# Usage: docker-compose -f examples/high-performance-llamacpp.yml up -d

services:
  llamacpp-server:
    image: ghcr.io/ggerganov/llama.cpp:server${GPU_ENABLED:+-cuda}
    container_name: devstral-llamacpp
    ports:
      - "${API_PORT:-8080}:8080"
    volumes:
      - ./models:/models:ro
    command: >
      --model /models/${MODEL_FILE:-devstral-model.gguf}
      --host 0.0.0.0
      --port 8080
      --ctx-size ${CONTEXT_SIZE:-8192}
      --threads ${THREADS:-8}
      --batch-size ${BATCH_SIZE:-1024}
      --n-gpu-layers ${GPU_LAYERS:-0}
      --rope-freq-base 10000
      --rope-freq-scale 1.0
      --numa
      --mlock
      --verbose
    deploy:
      resources:
        limits:
          memory: ${MEMORY_LIMIT:-16G}
        reservations:
          devices:
            - driver: nvidia
              count: ${GPU_COUNT:-1}
              capabilities: [gpu]
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/v1/models"]
      interval: 15s
      timeout: 10s
      retries: 5
      start_period: 30s
    logging:
      driver: "json-file"
      options:
        max-size: "50m"
        max-file: "3"

  openhands:
    image: docker.all-hands.dev/all-hands-ai/openhands:0.40
    container_name: devstral-openhands
    ports:
      - "${OPENHANDS_PORT:-3000}:3000"
    environment:
      - LLM_API_BASE=http://llamacpp-server:8080/v1
      - LLM_MODEL_NAME=${MODEL_NAME:-devstral}
      - SANDBOX_RUNTIME_CONTAINER_IMAGE=docker.all-hands.dev/all-hands-ai/runtime:0.40-nikolaik
      - SANDBOX_PERSIST_AFTER_START=false
      - WORKSPACE_MOUNT_PATH=/workspace
      - LLM_REQUEST_TIMEOUT=${REQUEST_TIMEOUT:-300}
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
      - ${WORKSPACE_PATH:-./workspace}:/workspace
    depends_on:
      llamacpp-server:
        condition: service_healthy
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 4G
        reservations:
          memory: 2G

  # Load balancer for multiple OpenHands instances (optional)
  nginx:
    image: nginx:alpine
    container_name: devstral-nginx
    ports:
      - "${NGINX_PORT:-80}:80"
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/nginx.conf:ro
    depends_on:
      - openhands
    restart: unless-stopped
    profiles:
      - loadbalancer

  # Redis for session management (optional)
  redis:
    image: redis:alpine
    container_name: devstral-redis
    ports:
      - "${REDIS_PORT:-6379}:6379"
    volumes:
      - redis_data:/data
    command: redis-server --appendonly yes --maxmemory 512mb --maxmemory-policy allkeys-lru
    restart: unless-stopped
    profiles:
      - cache

volumes:
  redis_data:

# Performance optimization examples:
# 
# CPU-only high performance:
# THREADS=16 BATCH_SIZE=512 CONTEXT_SIZE=4096 docker-compose -f examples/high-performance-llamacpp.yml up -d
#
# GPU acceleration:
# GPU_ENABLED=true GPU_LAYERS=35 CONTEXT_SIZE=8192 docker-compose -f examples/high-performance-llamacpp.yml up -d
#
# Multi-GPU setup:
# GPU_ENABLED=true GPU_COUNT=2 GPU_LAYERS=50 docker-compose -f examples/high-performance-llamacpp.yml up -d
#
# With load balancing:
# docker-compose -f examples/high-performance-llamacpp.yml --profile loadbalancer up -d --scale openhands=3